{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39164bitf79ddf7df00d49c9a3d5dba5c8382976",
   "display_name": "Python 3.9.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data from: ../data/mock_feature_store\nTraining Data Generation Complete!\nTotal training examples generated: 5,000\n\n--- SAMPLE OF FINAL MODEL-READY TRAINING DATA ---\n         userId   assetId           timestamp interaction_type  target region  \\\n0  b4968854-281  ab87f2b5 2024-01-19 10:50:00            CLICK       1     US   \n1  b00029af-f6f  8feccafb 2024-01-06 03:11:00            CLICK       1   APAC   \n2  b4968854-281  e9203c0c 2024-01-01 12:44:00             VIEW       0     US   \n3  2ffcbc90-799  877bad46 2024-01-28 21:24:00             VIEW       0   APAC   \n4  1f6ad6ff-5fa  8feccafb 2024-01-07 17:53:00            CLICK       1     EU   \n\n  risk_score account_tier favorite_sector   ticker   sector  market_cap_usd  \\\n0       HIGH          Pro          Gaming  COIN_45   Gaming    9.407328e+09   \n1       HIGH       Retail            DeFi   COIN_4     DeFi    3.381231e+10   \n2       HIGH          Pro          Gaming  COIN_22     DeFi    7.520183e+10   \n3     MEDIUM          Pro         Layer 1  COIN_28  Layer 1    4.030866e+10   \n4        LOW          Pro              AI   COIN_4     DeFi    3.381231e+10   \n\n   volatility_index                             asset_description_text  \n0             0.157  Highly competitive decentralized project focus...  \n1             0.293  Highly competitive decentralized project focus...  \n2             0.712  Highly competitive decentralized project focus...  \n3             0.574  Highly competitive decentralized project focus...  \n4             0.293  Highly competitive decentralized project focus...  \n\n--- DATA TYPES ---\nuserId                            object\nassetId                           object\ntimestamp                 datetime64[ns]\ninteraction_type                  object\ntarget                             int64\nregion                            object\nrisk_score                        object\naccount_tier                      object\nfavorite_sector                   object\nticker                            object\nsector                            object\nmarket_cap_usd                   float64\nvolatility_index                 float64\nasset_description_text            object\ndtype: object\n\n[Engineering] Processing Numerical Features...\n[Engineering] Categorical Features will be handled by Keras layers.\n[Engineering] Preparing Text Feature for NLP Embedding.\n\n--- SUMMARY OF PROCESSED TENSOR INPUTS ---\n\nUser Tower Inputs (Sample):\n  - region: dtype=object, shape=(5000,), sample='US'\n  - risk_score: dtype=object, shape=(5000,), sample='HIGH'\n  - account_tier: dtype=object, shape=(5000,), sample='Pro'\n  - favorite_sector: dtype=object, shape=(5000,), sample='Gaming'\n\nItem Tower Inputs (Sample):\n  - market_cap_usd: dtype=float64, shape=(5000,), sample='0.1217243448689738'\n  - volatility_index: dtype=float64, shape=(5000,), sample='0.0632126425285057'\n  - ticker: dtype=object, shape=(5000,), sample='COIN_45'\n  - sector: dtype=object, shape=(5000,), sample='Gaming'\n  - asset_description_text: dtype=object, shape=(5000,), sample='Highly competitive decentralized project focusing on Gaming infrastructure and governance.'\n\nTarget Shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from feature_store.offline_feature_loader import load_training_data\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_TOKENS = 10000  # Max vocabulary size for the Text Embedding\n",
    "SEQUENCE_LENGTH = 32 # Max length of the asset description text sequence\n",
    "\n",
    "# Define feature groups for clarity\n",
    "USER_CATEGORICAL_FEATURES = ['region', 'risk_score', 'account_tier', 'favorite_sector']\n",
    "ITEM_CATEGORICAL_FEATURES = ['ticker', 'sector']\n",
    "ITEM_NUMERICAL_FEATURES = ['market_cap_usd', 'volatility_index']\n",
    "ITEM_TEXT_FEATURE = 'asset_description_text'\n",
    "\n",
    "\n",
    "def preprocess_numerical_features(df: pd.DataFrame, feature_name: str, transformer=None) -> tuple[pd.Series, QuantileTransformer]:\n",
    "    \"\"\"\n",
    "    Applies a quantile transformation to numerical features to handle skewness\n",
    "    and ensure uniform distribution for better model performance.\n",
    "    \n",
    "    In a real MLOps system, the fitted transformer would be saved/versioned\n",
    "    and reused during serving to ensure consistency.\n",
    "    \"\"\"\n",
    "    data = df[[feature_name]].copy()\n",
    "    \n",
    "    if transformer is None:\n",
    "        # Fit the transformer for the first time (during training)\n",
    "        transformer = QuantileTransformer(output_distribution='uniform', n_quantiles=len(data))\n",
    "        transformed_data = transformer.fit_transform(data)\n",
    "    else:\n",
    "        # Reuse the fitted transformer (during serving/inference)\n",
    "        transformed_data = transformer.transform(data)\n",
    "\n",
    "    # Convert the numpy array back to a Series\n",
    "    return pd.Series(transformed_data.flatten(), name=f'{feature_name}_norm'), transformer\n",
    "\n",
    "\n",
    "def create_feature_pipeline(raw_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Orchestrates all feature engineering steps and organizes features \n",
    "    into separate dictionaries for the User and Item Towers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Feature Dictionaries to be returned\n",
    "    user_features = {}\n",
    "    item_features = {}\n",
    "    \n",
    "    # --- 2. Numerical Feature Processing (Quantile Normalization) ---\n",
    "    print(\"\\n[Engineering] Processing Numerical Features...\")\n",
    "    \n",
    "    # NOTE: In a production scenario, we would save the fitted transformers \n",
    "    # to disk and load them here. For this mock, we fit them during execution.\n",
    "    \n",
    "    for feature in ITEM_NUMERICAL_FEATURES:\n",
    "        normalized_series, _ = preprocess_numerical_features(raw_df, feature)\n",
    "        item_features[feature] = normalized_series.values\n",
    "    \n",
    "    # --- 3. Categorical Feature Processing (No transformation needed for pandas) ---\n",
    "    # We will let the Keras preprocessing layers (StringLookup, CategoryEncoding)\n",
    "    # handle the vocabulary and one-hot encoding *inside* the model definition.\n",
    "    print(\"[Engineering] Categorical Features will be handled by Keras layers.\")\n",
    "    \n",
    "    for feature in USER_CATEGORICAL_FEATURES:\n",
    "        user_features[feature] = raw_df[feature].astype(str).values\n",
    "        \n",
    "    for feature in ITEM_CATEGORICAL_FEATURES:\n",
    "        item_features[feature] = raw_df[feature].astype(str).values\n",
    "\n",
    "    # --- 4. Text Feature Processing (Tokenization and Sequence Length) ---\n",
    "    print(\"[Engineering] Preparing Text Feature for NLP Embedding.\")\n",
    "    # For now, we pass the raw text. The Keras TextVectorization layer will handle \n",
    "    # the heavy lifting (standardization, tokenization, vocab building) within the model.\n",
    "    item_features[ITEM_TEXT_FEATURE] = raw_df[ITEM_TEXT_FEATURE].astype(str).values\n",
    "\n",
    "    # --- 5. Target and Identifiers ---\n",
    "    # User and Item IDs are required for the final train/test split, \n",
    "    # but not as model input features themselves.\n",
    "    \n",
    "    processed_output = {\n",
    "        'user_features': user_features,\n",
    "        'item_features': item_features,\n",
    "        'target': raw_df['target'].values,\n",
    "        'user_ids': raw_df['userId'].values,\n",
    "        'asset_ids': raw_df['assetId'].values,\n",
    "    }\n",
    "    \n",
    "    return processed_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Load the raw joined data\n",
    "    raw_df = load_training_data()\n",
    "    \n",
    "    if not raw_df.empty:\n",
    "        # 2. Run the feature engineering pipeline\n",
    "        processed_data = create_feature_pipeline(raw_df)\n",
    "        \n",
    "        # 3. Print a summary of the processed output\n",
    "        print(\"\\n--- SUMMARY OF PROCESSED TENSOR INPUTS ---\")\n",
    "        \n",
    "        print(\"\\nUser Tower Inputs (Sample):\")\n",
    "        for k, v in processed_data['user_features'].items():\n",
    "            print(f\"  - {k}: dtype={v.dtype}, shape={v.shape}, sample='{v[0]}'\")\n",
    "\n",
    "        print(\"\\nItem Tower Inputs (Sample):\")\n",
    "        for k, v in processed_data['item_features'].items():\n",
    "            print(f\"  - {k}: dtype={v.dtype}, shape={v.shape}, sample='{v[0]}'\")\n",
    "            \n",
    "        print(f\"\\nTarget Shape: {processed_data['target'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}